## bit.ly의 1.usa.gov 데이터를 collections.Counter 클래스를 이용하여 표준시간대 세어보기

> import json
> import codecs
> path = 'C:/Users/ajtwl/PycharmProjects/P4DA/usagov_bitly_data2012-03-16-1331923249.txt'

> f = codecs.open(path, "r", "utf-8")
# txt를 json으로 바꿈 

> records = [json.loads(line) for line in f]

> print (records[0])
{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'c': 'US', 'nk': 1, 'tz': 'America/New_York', 'gr': 'MA', 'g': 'A6qOVH', 'h': 'wfLQtf', 'l': 'orofrog', 'al': 'en-US,en;q=0.8', 'hh': '1.usa.gov', 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991', 't': 1331923247, 'hc': 1331822918, 'cy': 'Danvers', 'll': [42.576698, -70.954903]}

> print (records[0]['tz'])
> America/New_York

# Collections.Counter 클래스
> from collections import Counter
> time_zones = [rec['tz'] for rec in records if 'tz' in rec]

> print(time_zones[:10])
['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo', 'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']

> counts = Counter(time_zones)
> print(counts)
Counter({'America/New_York': 1251, '': 521, 'America/Chicago': 400, 'America/Los_Angeles': 382, 'America/Denver': 191, 'Europe/London': 74, 'Asia/Tokyo': 37, 'Pacific/Honolulu': 36, 'Europe/Madrid': 35, 'America/Sao_Paulo': 33, 'Europe/Berlin': 28, 'Europe/Rome': 27, 'America/Rainy_River': 25, 'Europe/Amsterdam': 22, 'America/Phoenix': 20, 'America/Indianapolis': 20, 'Europe/Warsaw': 16, 'America/Mexico_City': 15, 'Europe/Paris': 14, 'Europe/Stockholm': 14, 'America/Vancouver': 12, 'Pacific/Auckland': 11, 'Asia/Hong_Kong': 10, 'Europe/Prague': 10, 'Europe/Moscow': 10, 'Europe/Helsinki': 10, 'America/Puerto_Rico': 10, 'Europe/Oslo': 10, 'America/Montreal': 9, 'Asia/Calcutta': 9, 'Asia/Istanbul': 9, 'Europe/Lisbon': 8, 'Asia/Bangkok': 6, 'Europe/Vienna': 6, 'Australia/NSW': 6, 'Chile/Continental': 6, 'America/Edmonton': 6, 'Europe/Athens': 6, 'Europe/Copenhagen': 5, 'America/Anchorage': 5, 'Europe/Budapest': 5, 'Asia/Seoul': 5, 'Europe/Brussels': 4, 'Asia/Beirut': 4, 'America/Halifax': 4, 'Europe/Bucharest': 4, 'Europe/Zurich': 4, 'America/Winnipeg': 4, 'Asia/Dubai': 4, 'Asia/Kuala_Lumpur': 3, 'Europe/Dublin': 3, 'Asia/Jerusalem': 3, 'Asia/Karachi': 3, 'America/Bogota': 3, 'Europe/Bratislava': 3, 'Africa/Cairo': 3, 'America/Managua': 3, 'Asia/Harbin': 3, 'Asia/Jakarta': 3, 'Africa/Ceuta': 2, 'Europe/Malta': 2, 'America/Recife': 2, 'Europe/Riga': 2, 'Europe/Belgrade': 2, 'America/Chihuahua': 2, 'Europe/Vilnius': 2, 'America/Guayaquil': 2, 'Asia/Amman': 2, 'Asia/Nicosia': 1, 'America/Mazatlan': 1, 'Europe/Skopje': 1, 'Asia/Novosibirsk': 1, 'Europe/Sofia': 1, 'Europe/Ljubljana': 1, 'America/Monterrey': 1, 'America/Argentina/Buenos_Aires': 1, 'Asia/Yekaterinburg': 1, 'Asia/Manila': 1, 'America/Caracas': 1, 'Asia/Riyadh': 1, 'America/Montevideo': 1, 'America/Argentina/Mendoza': 1, 'Europe/Uzhgorod': 1, 'Australia/Queensland': 1, 'America/Costa_Rica': 1, 'America/Lima': 1, 'Asia/Pontianak': 1, 'Africa/Lusaka': 1, 'Africa/Johannesburg': 1, 'America/St_Kitts': 1, 'America/Santo_Domingo': 1, 'America/Argentina/Cordoba': 1, 'Asia/Kuching': 1, 'Europe/Volgograd': 1, 'America/La_Paz': 1, 'Africa/Casablanca': 1, 'America/Tegucigalpa': 1})

> print(counts.most_common(10))
[('America/New_York', 1251), ('', 521), ('America/Chicago', 400), ('America/Los_Angeles', 382), ('America/Denver', 191), ('Europe/London', 74), ('Asia/Tokyo', 37), ('Pacific/Honolulu', 36), ('Europe/Madrid', 35), ('America/Sao_Paulo', 33)]


==========================================================================================================================
# pandas를 이용한 표준 시간대 세어보기 
> import json
> import codecs
> from pandas import DataFrame, Series
> import pandas as pd

> path = 'C:/Users/ajtwl/PycharmProjects/P4DA/usagov_bitly_data2012-03-16-1331923249.txt'
> f = codecs.open(path, "r", "utf-8")
> records = [json.loads(line) for line in f]

> frame = DataFrame(records)

> print(frame.info())
# r에서 str와 비슷하다.

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3560 entries, 0 to 3559
Data columns (total 18 columns):
_heartbeat_    120 non-null float64
a              3440 non-null object
al             3094 non-null object
c              2919 non-null object
cy             2919 non-null object
g              3440 non-null object
gr             2919 non-null object
h              3440 non-null object
hc             3440 non-null float64
hh             3440 non-null object
kw             93 non-null object
l              3440 non-null object
ll             2919 non-null object
nk             3440 non-null float64
r              3440 non-null object
t              3440 non-null float64
tz             3440 non-null object
u              3440 non-null object
dtypes: float64(4), object(14)
memory usage: 500.7+ KB
None

> print(frame['tz'][:10])
# r에서 subset으로 'tz'만 추린것과 같음 

0     America/New_York
1       America/Denver
2     America/New_York
3    America/Sao_Paulo
4     America/New_York
5     America/New_York
6        Europe/Warsaw
7                     
8                     
9                     
Name: tz, dtype: object

> tz_counts = frame['tz'].value_counts()
# r에서 table과 비슷하다.
> print(tz_counts[:10])

America/New_York       1251
                        521
America/Chicago         400
America/Los_Angeles     382
America/Denver          191
Europe/London            74
Asia/Tokyo               37
Pacific/Honolulu         36
Europe/Madrid            35
America/Sao_Paulo        33
Name: tz, dtype: int64

> clean_tz = frame['tz'].fillna('Missing')
> clean_tz[clean_tz==''] = 'Unknown'
# null 값을 Unknown으로 설정

> tz_counts = clean_tz.value_counts()
> print(tz_counts[:10])

America/New_York       1251
Unknown                 521
America/Chicago         400
America/Los_Angeles     382
America/Denver          191
Missing                 120
Europe/London            74
Asia/Tokyo               37
Pacific/Honolulu         36
Europe/Madrid            35
Name: tz, dtype: int64

> print(frame.a)
0       Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...
1                                  GoogleMaps/RochesterNY
2       Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ...
3       Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)...
4       Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...

> result = Series([x.split()[0] for x in frame.a.dropna()])

# dropna : Nan을 제거
# 앞글자만 split를 하겠다라는 의미를 담고 있다. 

>>>  Series란 ? 
사실 pandas의 Series는 어떤 면에서는 파이썬의 리스트와 비슷하고 어떤 면에서는 파이썬의 딕셔너리와 닮은 알쏭달쏭한 자료구조입니다. 
지금부터 간단히 코드를 작성해가면서 Series라는 자료구조를 공부해 봅시다.
pandas의 Series는 사실 클래스입니다. 생성자로 다음과 같이 파이썬 리스트를 넘겨주면 해당 값에 포함하는 Series 객체를 생성해줍니다.
from pandas import Series, DataFrame
kakao = Series([92600, 92400, 92100, 94300, 92300])
print(kakao)
Series 객체를 생성할 때 따로 인덱스를 지정하지 않았다면 기본적으로 0부터 시작하는 정숫값으로 인덱싱됩니다.

0    92600
1    92400
2    92100
3    94300
4    92300
dtype: int64

> print(result[:5])

0               Mozilla/5.0
1    GoogleMaps/RochesterNY
2               Mozilla/4.0
3               Mozilla/5.0
4               Mozilla/5.0
dtype: object

> print(result.value_counts())

Mozilla/5.0                                          2594
Mozilla/4.0                                           601
GoogleMaps/RochesterNY                                121
Opera/9.80                                             34
TEST_INTERNET_AGENT                                    24

---------------------------------------------------------------------------------------------------------------------
> import json
> import codecs
> from pandas import DataFrame, Series
> import pandas as pd
> import numpy as np 
# numpy 추가 

> path = 'C:/Users/ajtwl/PycharmProjects/P4DA/usagov_bitly_data2012-03-16-1331923249.txt'
> f = codecs.open(path, "r", "utf-8")
> records = [json.loads(line) for line in f]
> frame = DataFrame(records)
> cframe = frame[frame.a.notnull()]
# NA값 제거
> operating_system = np.where(cframe['a'].str.contains('Window'),'Windows','Not Windows')
# Window가 있으면 Windows 출력, 없으면 Not Windows 출력 

> print(operating_system[:5])
['Windows' 'Not Windows' 'Windows' 'Not Windows' 'Windows']

> by_tz_os = cframe.groupby(['tz',operating_system])
# groupby를 통해 tz와 operating_system인 그룹을 만들었다.

> agg_count = by_tz_os.size().unstack().fillna(0)
# size : 그룹별 갯수 집계, unstack : 결과를 표로 재배치

> print(agg_count[:10])

                                Not Windows  Windows
tz                                                  
                                      245.0    276.0
Africa/Cairo                            0.0      3.0
Africa/Casablanca                       0.0      1.0
Africa/Ceuta                            0.0      2.0
Africa/Johannesburg                     0.0      1.0
Africa/Lusaka                           0.0      1.0
America/Anchorage                       4.0      1.0
America/Argentina/Buenos_Aires          1.0      0.0
America/Argentina/Cordoba               0.0      1.0
America/Argentina/Mendoza               0.0      1.0

> indexer = agg_count.sum(1).argsort()
# 전체 표준시간대의 순위를 모아보자. argsort()는 뒤로 갈수록 클수가 나오는 오름차순에 순서만을 출력한다.
# a = np.array([42, 38, 12, 25])
# j = np.argsort(a)
# j
# 결과 값 : array([2, 3, 1, 0])

> print(indexer)

tz
                                  24
Africa/Cairo                      20
Africa/Casablanca                 21
Africa/Ceuta                      92
Africa/Johannesburg               87

> count_subset = agg_count.take(indexer)[-10:]
# take를 사용해 행을 오름차순으로 정렬된 순서 그대로 선택하고 마지막 10개 행만 잘라낸다. 

> print(count_subset)

                     Not Windows  Windows
tz                                       
America/Sao_Paulo           13.0     20.0
Europe/Madrid               16.0     19.0
Pacific/Honolulu             0.0     36.0
Asia/Tokyo                   2.0     35.0
Europe/London               43.0     31.0
America/Denver             132.0     59.0
America/Los_Angeles        130.0    252.0
America/Chicago            115.0    285.0
                           245.0    276.0
America/New_York           339.0    912.0

-------------------------------------------

> import pandas as pd
> import os
> encoding ='latin1'
> mpath = os.path.expanduser('C:/Users/ajtwl/PycharmProjects/P4DA/movies.dat')
> upath = os.path.expanduser('C:/Users/ajtwl/PycharmProjects/P4DA/users.dat')
> rpath = os.path.expanduser('C:/Users/ajtwl/PycharmProjects/P4DA/ratings.dat')
>> expanduser 
입력받은 경로안의 "~"를 현재 사용자 디렉터리의 절대경로로 대체합니다.
"~"에 붙여서 <사용자명>을 붙이면 원하는 사용자 경로로 대체됩니다.

> unames = ['user_id','gender','age','occupation','zip']
> rnames = ['user_id','movie_id','rating','timestamp']
> mnames = ['movie_id','title','genres']
# 열 Name 정하기 

> users= pd.read_csv(upath, sep= '::', header = None, names =unames, encoding=encoding)
> rating= pd.read_csv(rpath, sep= '::', header = None, names =rnames, encoding=encoding)
> movies= pd.read_csv(mpath, sep= '::', header = None, names =mnames, encoding=encoding)
# 구분자 : "::", header는 None, name : 열은 아까 설정했던 names, encoding은 위에 있는 'latin1'

> data = pd.merge(pd.merge(rating,users),movies)
# merge를 하면 ratings 테이블과 users 테이블을 병합하고 그 결과를 다시 movies 테이블과 병합한다. 
# pandas는 병합하려는 두 테이블에서 중복되는 열의 이름을 키로 사용한다. 
# 여기서는 user_id가 중복되어 있기 때문에 merge를 할 때 user_id를 통해 inner join을 할 수 있다. 
# 그리고 그 결과에서 movie_id끼리 inner join을 실행한다.

> print(data)
         user_id  movie_id  rating   timestamp gender  age  occupation    zip   title					                           genres 
0              1      1193       5   978300760      F    1          10  48067   One Flew Over the Cuckoo's Nest (1975)   Drama
1              2      1193       5   978298413      M   56          16  70072   One Flew Over the Cuckoo's Nest (1975)   Drama
2             12      1193       4   978220179      M   25          12  32793   One Flew Over the Cuckoo's Nest (1975)   Drama
3             15      1193       4   978199279      M   25           7  22903   One Flew Over the Cuckoo's Nest (1975)   Drama
4             17      1193       5   978158471      M   50           1  95350   One Flew Over the Cuckoo's Nest (1975)   Drama
5             18      1193       4   978156168      F   18           3  95825   One Flew Over the Cuckoo's Nest (1975)   Drama
6             19      1193       5   982730936      M    1          10  48073   One Flew Over the Cuckoo's Nest (1975)   Drama

> mean_ratings =  data.pivot_table('rating', index='title',columns= 'gender',aggfunc='mean')
# 성별에 따른 각 영화의 평균 평점
# 'rating' : 데이터 셋 
# index : 사용하고자 하는 열
# columns : 사용하고자 하는 열 
# aggfunc = mean : 평균

> print(mean_ratings)
gender                                                 F         M
title                                                             
$1,000,000 Duck (1971)                          3.375000  2.761905
'Night Mother (1986)                            3.388889  3.352941
'Til There Was You (1997)                       2.675676  2.733333

> ratings_by_title = data.groupby('title').size()
# 영화 제목으로 그룹핑 

> print(ratings_by_title)
title
$1,000,000 Duck (1971)                              37
'Night Mother (1986)                                70
'Til There Was You (1997)                           52
'burbs, The (1989)                                 303
...And Justice for All (1979)                      199

> active_titles = ratings_by_title.index[ratings_by_title >= 250]
# 250건 이상의 평점 정보 있는 영화만 추려보자.

> print(active_titles)
Index([''burbs, The (1989)', '10 Things I Hate About You (1999)',
       '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)',
       '13th Warrior, The (1999)', '2 Days in the Valley (1996)',
       '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)',

> mean_ratings = mean_ratings.ix[active_titles]
# 250건 이상의 평점 정보가 있는 영화에 대한 색인은 mean_ratings에서 항목을 선택하기 위해 사용한다. 

> print(mean_ratings)
gender                                                     F         M
title                                                                 
'burbs, The (1989)                                  2.793478  2.962085
10 Things I Hate About You (1999)                   3.646552  3.311966
101 Dalmatians (1961)                               3.791444  3.500000
101 Dalmatians (1996)                               3.240000  2.911215
12 Angry Men (1957)                                 4.184397  4.328421

> top_female_ratings = mean_ratings.sort_index(by='F',ascending=False)
# F열을 내림차순으로 정렬한다. 

> print(top_female_ratings)

gender                                                     F         M
title                                                                 
Close Shave, A (1995)                               4.644444  4.473795
Wrong Trousers, The (1993)                          4.588235  4.478261
Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)       4.572650  4.464589
Wallace & Gromit: The Best of Aardman Animation...  4.563107  4.385075

> mean_ratings['diff'] = mean_ratings['M']-mean_ratings['F']
> sorted_by_diff = mean_ratings.sort_values(by='diff')
# sort_values는 diff로 오름차순한다.

> print(sorted_by_diff[:15])
gender                                        F         M      diff
title                                                              
Dirty Dancing (1987)                   3.790378  2.959596 -0.830782
Jumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359
Grease (1978)                          3.975265  3.367041 -0.608224
Little Women (1994)                    3.870588  3.321739 -0.548849
Steel Magnolias (1989)                 3.901734  3.365957 -0.535777

> rating_std_by_title = data.groupby('title')['rating'].std()
# 평점의 표준편차 

> print(rating_std_by_title)
title
$1,000,000 Duck (1971)                            1.092563
'Night Mother (1986)                              1.118636
'Til There Was You (1997)                         1.020159
'burbs, The (1989)                                1.107760

> rating_std_by_title = rating_std_by_title.ix[active_titles]
# 250건 이상의 평점 정보가 있는 것만을 나타내는 active_titles인것만을 추려 표준편차를 계산

> print(rating_std_by_title)
title
'burbs, The (1989)                                                    1.107760
10 Things I Hate About You (1999)                                     0.989815
101 Dalmatians (1961)                                                 0.982103
101 Dalmatians (1996)                                                 1.098717
12 Angry Men (1957)                                                   0.812731

> top_female_ratings = mean_ratings.sort_index(by='F',ascending=False)
# 내림차순

> print(top_female_ratings)
gender                                                     F         M  
title                                                                    
Close Shave, A (1995)                               4.644444  4.473795   
Wrong Trousers, The (1993)                          4.588235  4.478261   
Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)       4.572650  4.464589   
Wallace & Gromit: The Best of Aardman Animation...  4.563107  4.385075   
Schindler's List (1993)                             4.562602  4.491415  

> top_female_ratings = mean_ratings.sort_index()
# sort_index()를 할 경우에는 title의 알파벳 순서로 정렬되고 by=F와 ascending = False를 통해 정렬할 수 있다. 

> print(top_female_ratings)
gender                                                     F         M  \
title                                                                    
'burbs, The (1989)                                  2.793478  2.962085   
10 Things I Hate About You (1999)                   3.646552  3.311966   
101 Dalmatians (1961)                               3.791444  3.500000   
101 Dalmatians (1996)                               3.240000  2.911215   
12 Angry Men (1957)                                 4.184397  4.328421   

----------------------------------------------------------------------------------------------------------------------------
# 신생아 이름
> import pandas as pd

> names1880 = pd.read_csv('C:/Users/ajtwl/PycharmProjects/P4DA/yob1880.txt', names =['name','sex','births'])
# csv 읽기
> print(names1880)
           name sex  births
0          Mary   F    7065
1          Anna   F    2604
2          Emma   F    2003
3     Elizabeth   F    1939
4        Minnie   F    1746

> print(names1880.groupby('sex').births.sum())
# names1880 데이터에 sex를 groupby를 하고 births 열에서 sum을 해라

sex
F     90993
M    110493
Name: births, dtype: int64

------------------------------------------------------------------------------------------------------------------------------
# 여러개의 이름이 비슷한 데이터를 하나의 변수에 입력

> import pandas as pd
> years = range(1881,1970)
> pieces = []
> columns = ['name','sex','births']

> for year in years :
    path = 'C:/Users/ajtwl/PycharmProjects/P4DA/yob%d.txt' %year
    # %d를 통해 %year 값을 입력한다.
    frame = pd.read_csv(path,names = columns)
    # path 데이터에서 names는 열을 의미하며 열에 columns의 값을 읽어서 frame에 저장

    frame['year']= year
    # year 값을 frame['year']에 저장한다.
    pieces.append(frame)
    # frame을 추가한다.

    names =pd.concat(pieces, ignore_index=True)
# concat을 통해 여러개의 데이터프레임 pieces를 합친다. 
# 읽어온 행의 순서는 몰라도 되므로 concat 메서드에 ignore_index =True를 인자로 전달해야 한다.

> print(names)
             name sex  births  year
0            Mary   F    6919  1881
1            Anna   F    2698  1881
2            Emma   F    2034  1881
3       Elizabeth   F    1852  1881

--------------------------------------------------------------------------------------------------------------------------------

> def add_prob(group):
    births =group.births.astype(float)
    group['prop'] = births/ births.sum()
    return group
> names = names.groupby(['year','sex']).apply(add_prob)
# name에서 groupby를 year열과, sex열의 그룹을 만들고 add_prob 함수를 적용한다. 

> print(np.allclose(names.groupby(['year','sex']).prop.sum(),1))
# prop.sum의 값이 1이면 true, 아니면 false

> print(names)
             name sex  births  year      prop
0            Mary   F    6919  1881  0.075243
1            Anna   F    2698  1881  0.029340
2            Emma   F    2034  1881  0.022120

> total_births = names.pivot_table('births',index ='year',columns='sex',aggfunc=sum)
# birth : 출생수, index : 년도별로 인덱스, columns는 성별로 구분, 적용할 함수는 sum

> print(total_births.tail())
sex         F        M
year                  
1965  1764815  1861138
1966  1691650  1783577
1967  1650576  1744222
1968  1639977  1738585
1969  1686705  1789326

> pieces = []
> for year, group in names.groupby(['year','sex']):
    pieces.append(group.sort_values(by='births', ascending=False)[:1000])
    # pieces에 group의 sort_values를 통해 births 값을 내림차순으로 1000개 가져옴
    top1000=pd.concat(pieces,ignore_index=True)
    # 연도별, 성별에 따른 빈도수가 가장 높은 이름 1000개를 추출하자.
    # 여기에서 append만 할 경우에는 각각 따로 값들이 나온다. 즉, 각각의 데이터 프레임으로 저장되기 때문에 하나의 데이터프레임이 되지 않는다.
    따라서 concat을 통해 append로 된 각각의 데이터 프레임을 하나로 연결해준다. r을 본다면 rbind와 같다. 
    # axis=0, 0: 위+아래로 합치기, 1: 왼쪽+오른쪽으로 합치기
> print(pieces)
664261      Derrell   M      53  1967  0.000030
664262       Dickie   M      53  1967  0.000030
664263    Ferdinand   M      53  1967  0.000030
664264       Franco   M      53  1967  0.000030

[1000 rows x 5 columns],              name sex  births  year      prop
667815       Lisa   F   49523  1968  0.030197
667816   Michelle   F   33206  1968  0.020248
667817   Kimberly   F   31908  1968  0.019456
# 각각의 데이터 프레임으로 저장되어 있다. 

> print(top1000)
             name sex  births  year      prop
0            Mary   F    6919  1881  0.075243
1            Anna   F    2698  1881  0.029340
2            Emma   F    2034  1881  0.022120
3       Elizabeth   F    1852  1881  0.020140
4        Margaret   F    1658  1881  0.018031
# 하나의 데이터 프레임으로 저장되어 있다. 


# ~70page 완료 
